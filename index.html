<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Main Step: Edit below title -->
    <title>TODO: Paper Title</title>

    <!-- SEO Meta Tags -->
    <meta name="description" content="" />
    <meta
      name="keywords"
      content="research papers, academic papers, AI research, robotics research, machine learning, published papers, scientific papers, academic journals"
    />

    <link
      rel="icon"
      href="Assets/Favicon/icons8-research-color-16.png"
      sizes="16x16"
      type="image/png"
    />
    <link
      rel="icon"
      href="Assets/Favicon/icons8-research-color-32.png"
      sizes="32x32"
      type="image/png"
    />
    <link
      rel="icon"
      href="Assets/Favicon/icons8-research-color-70.png"
      sizes="70x70"
      type="image/png"
    />
    <link
      rel="icon"
      href="Assets/Favicon/icons8-research-color-72.png"
      sizes="72x72"
      type="image/png"
    />
    <link
      rel="icon"
      href="Assets/Favicon/icons8-research-color-96.png"
      sizes="96x96"
      type="image/png"
    />

    <link rel="stylesheet" type="text/css" href="style.css" />
  </head>
  <body>
    <nav class="nav">
      <a href="#abstract">Abstract</a>
      <a href="#methodology">Methodology</a>
      <a href="#results">Results</a>
      <a href="#supplementary_videos">Supplementary Videos</a>
    </nav>


    <div class="header">
      <h1>Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies</h1>
      <div class="authors">
        <p>
          <a href="https://hamrel-cxu.github.io/">Chen Xu</a><sup>1</sup>,
          <a href="https://www.linkedin.com/in/tony-nguyen-22389669/">Tony Khuong Nguyen</a><sup>1</sup>,
          <a href="#">Emma Dixon</a><sup>1</sup>,
          <a href="#">Christopher Rodriguez</a><sup>1</sup>,
          <a href="#">Patrick Miller</a><sup>1</sup>,
          <a href="https://scholar.google.com.au/citations?hl=en&user=1Vqlm0kAAAAJ&view_op=list_works">Robert Lee</a><sup>1</sup>,
          <a href="https://www.linkedin.com/in/paarth-shah-bb6743b1/">Paarth Shah</a><sup>1</sup>,
          <a href="https://scholar.google.com/citations?user=2xjjS3oAAAAJ&hl=en&oi=ao">Rares Andrei Ambrus</a><sup>1</sup>,
          <a href="https://harukins.github.io/">Haruki Nishimura</a><sup>1</sup>,
          <a href="https://mashaitkina.weebly.com/">Masha Itkina</a><sup>1</sup>
        </p>
        <p>
          <sup>1</sup>Toyota Research Institute &nbsp;&nbsp
        </p>
      </div>
    </div>


    <div class="buttons">
      <a href="#" target="_blank">Paper</a>
      <a href="https://github.com/chen-xu-pi/FAIL-Detect" target="_blank">Code</a>
      <a href="#bibtex">BibTex</a>
    </div>

    <div class="content-section" id="figure1">
      <img src="Assets/motivation.png" style="display: block; margin: 0 auto; width: 100%;">
      <p style="text-align: center;">How to detect failures during rollouts without a priori knowledge of diverse failure modes and the collection of failure data?</p>
    </div>

    <div class="abstract" id="abstract">
      <h2>Abstract</h2>
      <p>Recent years have witnessed impressive robotic manipulation systems driven by advances in imitation learning and generative modeling, such as diffusion- and flow-based approaches. As robot policy performance increases, so does the complexity and time horizon of achievable tasks, inducing unexpected and diverse failure modes that are difficult to predict a priori. To enable trustworthy policy deployment in safety-critical human environments, reliable runtime failure detection becomes important during policy inference. However, most existing failure detection approaches rely on prior knowledge of failure modes and require failure data during training, which imposes a significant challenge in practicality and scalability. In response to these limitations, we present FAIL-Detect, a modular two-stage approach for failure detection in imitation learning-based robotic manipulation. To accurately identify failures from successful training data, we frame the problem as sequential out-of-distribution (OOD) detection. We first distill policy inputs and outputs into scalar signals that correlate with policy failures and capture epistemic uncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile framework for uncertainty quantification with statistical guarantees. Empirically, we thoroughly investigate both learned and post-hoc scalar signal candidates on diverse robotic manipulation tasks. Our experiments show learned signals based on adapted random networks and a novel flow-based density estimator to be most effective. Furthermore, our method detects failures more accurately and faster than state-of-the-art (SOTA) failure detection baselines. These results highlight the potential of FAIL-Detect to enhance the safety and reliability of imitation learning-based robotic systems as they progress toward real-world deployment.</p>
    </div>

    <div class="content-section" id="methodology">
      <h2>Methodology</h2>
      <img src="Assets/method.png" style="display: block; margin: 0 auto; width: 100%;">
      <p>We propose a two-stage approach to failure detection as demonstrated above:</p>
      <ul>
        <li> <b>(Left - Stage I)</b> Multi-view camera images and robot states are distilled into failure detection scalar scores. Images are first passed through a feature extractor and then, along with robot states, constitute observations. Both observations and generated future robot actions can serve as inputs to a score network. This network outputs scalar scores that capture characteristics of successful demonstration data.</li>
        <li> <b>(Middle - Stage II)</b> Scores from a calibration set of successful rollouts are then used to compute a mean and band width to build the time-varying conformal prediction threshold.</li>
        <li> <b>(Right - Runtime Failure Detection)</b> A successful trajectory (bottom) has scores that consistently remain below the threshold. When a failure occurs (top), such as failure to fold the towel, the score spikes above the threshold, triggering failure detection (red box).</li>
      </ul>
    </div>

    <div class="content-section" id="results">
      <h2>Results</h2>
      <p>TODO: Add Results</p>
      <ul>
        <li>TODO</li>
        <li>TODO</li>
      </ul>
    </div>

    <div class="content-section" id="supplementary_videos">
      <h2>Supplementary Videos</h2>
      <video src="Assets/video.mp4" controls style="display: block; margin: 0 auto; width: 100%;"></video>
    </div>

    <div class="bibtex-section" id="bibtex">
      <h2>BibTeX</h2>
      <button class="bibtex-copy-button" onclick="copyBibTeX()">
        Copy to Clipboard
      </button>
      <pre>
        <!-- Please edit only below details -->
        <code class="language-bibtex">
          @inproceedings{TODO: YourPaperCitation,
            title={TODO: Paper Title},
            author={[TODO: Author Names]},
            booktitle={[TODO: Conference Name]},
            year={TODO: 2024},
            pages={[pages]}
          }
        </code>
      </pre>
    </div>
  </body>
</html>
